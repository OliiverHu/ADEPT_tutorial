{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Use ADEPT on customized ST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### packages loading in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import os\n",
    "from imputation.impute import impute_\n",
    "import GAAE\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "import time\n",
    "import seaborn as sns \n",
    "from GAAE.utils import impute, DE_num_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build up your own loader function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example for loading a customized dataset. You could provide cell annotation to it optionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_customized(data_path):\n",
    "    \"\"\"this is an example for reading .h5 file, if you have different saving format, please refer to scanpy or implement your own function to read in\"\"\"\n",
    "    ad = sc.read_visium(path=data_path, count_file='_filtered_feature_bc_matrix.h5', load_images=True)\n",
    "    ad.var_names_make_unique()\n",
    "\n",
    "    \"\"\"comment out this if you dont have gt/cell annotation. This is an example for reading gt which is stored in .txt file\"\"\"\n",
    "    gt_dir = os.path.join(\"./\", \"./\", 'gt')\n",
    "    gt_df = pd.read_csv(os.path.join(gt_dir, 'tissue_positions_list_GTs.txt'), sep=',', header=None, index_col=0)\n",
    "    ad.obs['original_clusters'] = gt_df.loc[:, 6]\n",
    "    keep_bcs = ad.obs.dropna().index\n",
    "    ad = ad[keep_bcs].copy()\n",
    "    ad.obs['original_clusters'] = ad.obs['original_clusters'].astype(int).astype(str)\n",
    "    \n",
    "    return ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the tutorial runnable, I will still use the DLPFC dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_customized(data_path):\n",
    "    # 151507, ..., 151676 12 in total\n",
    "    ad = sc.read_visium(path=\"/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12/151673\", count_file='151673_filtered_feature_bc_matrix.h5', load_images=True)\n",
    "    ad.var_names_make_unique()\n",
    "\n",
    "    return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num_calc(args_, comp_):\n",
    "    if comp_ is not None:\n",
    "        return comp_\n",
    "    print(\"optimizing minimum filter number\")\n",
    "    # input_dir = os.path.join(args_.data_dir, args_.input_data)\n",
    "    data_name = args_.input_data\n",
    "    data_path = args_.data_dir\n",
    "    if data_name:\n",
    "        adata = load_customized(data_path)\n",
    "    else:\n",
    "        print(\"exception in dataname\")\n",
    "        exit(-1)\n",
    "    adata.var_names_make_unique()\n",
    "\n",
    "    for temp_count in range(5, 150):\n",
    "        sc.pp.filter_genes(adata, min_counts=temp_count)\n",
    "        try:\n",
    "            X = adata.X.todense()\n",
    "        except:\n",
    "            X = adata.X\n",
    "        if np.count_nonzero(X)/(X.shape[0]*X.shape[1]) > args_.filter_nzr:\n",
    "            return temp_count\n",
    "    return 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(args_, gene_min_count):\n",
    "    print(\"initializing spatial transcriptomic data\")\n",
    "    data_path = args_.data_dir\n",
    "    data_name = args_.input_data\n",
    "    if data_name:\n",
    "        adata_ = load_customized(data_path)\n",
    "        adata_ori_ = adata_\n",
    "        if args_.use_preprocessing:\n",
    "            # Normalization\n",
    "            sc.pp.filter_genes(adata_, min_counts=gene_min_count)\n",
    "            if args_.use_hvgs != 0:\n",
    "                sc.pp.highly_variable_genes(adata_, flavor=\"seurat_v3\", n_top_genes=args_.use_hvgs)\n",
    "            sc.pp.normalize_total(adata_, target_sum=1e4)\n",
    "            sc.pp.log1p(adata_)\n",
    "        else:\n",
    "            sc.pp.filter_genes(adata_, min_counts=gene_min_count)\n",
    "    else:\n",
    "        print(\"exception in data name\")\n",
    "        exit(-1)\n",
    "    return adata_, adata_ori_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argument parser: change the necessary args to your customized value by setting args.data_dir='/path/to/your/input/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default=\"./\", help=\"root dir for input data\")\n",
    "parser.add_argument('--gt_dir', type=str, default=\"./\", help=\"root dir for data ground truth\")\n",
    "parser.add_argument('--input_data', type=str, default=\"151673\", help=\"input data section id\")\n",
    "parser.add_argument('--impute_cluster_num', type=str, default=\"7\", help=\"diff cluster numbers for imputation\")\n",
    "parser.add_argument('--cluster_num', type=int, default=7, help=\"input data cluster number\")\n",
    "parser.add_argument('--radius', type=int, default=150, help=\"input data radius\")\n",
    "parser.add_argument('--no_de', type=int, default=0, help='switch on/off DEG selection module')\n",
    "parser.add_argument(\"--use_mean\", type=int, default=0, help=\"use mean value in de list or not\")\n",
    "parser.add_argument(\"--impute_runs\", type=int, default=1, help=\"time of runs for imputation\")\n",
    "parser.add_argument(\"--runs\", type=int, default=2, help=\"total runs for the data\")\n",
    "parser.add_argument('--use_hvgs', type=int, default=3000, help=\"select highly variable genes before training\")\n",
    "parser.add_argument('--use_preprocessing', type=int, default=1, help='use preprocessed input or raw input')\n",
    "parser.add_argument('--save_fig', type=int, default=1, help='saving output visualization')\n",
    "parser.add_argument('--de_nzr_min', type=float, default=0.299, help='suggested min nzr threshold after de selection')\n",
    "parser.add_argument('--de_nzr_max', type=float, default=0.399, help='suggested max nzr threshold after de selection')\n",
    "parser.add_argument('--use_gpu_id', type=str, default='0', help='use which GPU, only applies when you have multiple gpu')\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.impute_cluster_num = args.impute_cluster_num.split(\",\")  # [\"5\", \"6\", \"7\"]\n",
    "args.filter_nzr = 0.15\n",
    "args.filter_num = 5\n",
    "root_d = args.data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing to get the parameters for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spatial transcriptomic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/anaconda3/envs/test/lib/python3.7/site-packages/anndata/_core/anndata.py:1830: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing top DEs before imputation\n",
      "original non-zero =  0.12213016680918756\n",
      "(3639, 18032)\n",
      "DE topk =  50\n",
      "section id =  151673\n",
      "------Calculating spatial graph...\n",
      "The graph contains 21124 edges, 3639 cells.\n",
      "5.8049 neighbors per cell on average.\n",
      "Size of Input:  (3639, 3000)\n",
      "GAAE(\n",
      "  (conv1): GATConv(3000, 512, heads=1)\n",
      "  (conv2): GATConv(512, 30, heads=1)\n",
      "  (conv3): GATConv(30, 512, heads=1)\n",
      "  (conv4): GATConv(512, 3000, heads=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 85.23it/s]\n",
      "R[write to console]:                    __           __ \n",
      "   ____ ___  _____/ /_  _______/ /_\n",
      "  / __ `__ \\/ ___/ / / / / ___/ __/\n",
      " / / / / / / /__/ / /_/ (__  ) /_  \n",
      "/_/ /_/ /_/\\___/_/\\__,_/____/\\__/   version 5.4.10\n",
      "Type 'citation(\"mclust\")' for citing this R package in publications.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting ...\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From cffi callback <function _processevents at 0x7fd6f558a830>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yunfei/anaconda3/envs/test/lib/python3.7/site-packages/rpy2/rinterface_lib/callbacks.py\", line 275, in _processevents\n",
      "    @ffi_proxy.callback(ffi_proxy._processevents_def,\n",
      "  File \"/home/yunfei/anaconda3/envs/test/lib/python3.7/site-packages/rpy2/rinterface.py\", line 92, in _sigint_handler\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting DEs after first round of clustering ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/Spatial/ADEPT/GAAE/Train_ADEPT_modified.py:508: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  gene_list = adata.var.index[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18032, 1182) (18032, 2457)\n",
      "label =  1\n"
     ]
    }
   ],
   "source": [
    "filter_num = filter_num_calc(args, args.filter_num)\n",
    "\n",
    "adata, adata_ori = initialize(args, filter_num)\n",
    "\n",
    "if os.path.exists('./cache/' + args.data_dir.split('/')[-2] + args.input_data + '.txt'):\n",
    "    with open('./cache/' + args.data_dir.split('/')[-2] + '.txt', 'r') as fp:\n",
    "        line = fp.readlines()[0]\n",
    "        split_ = line.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "    print(\"previously cached de list = \", de_top_k_list)\n",
    "else:\n",
    "    de_top_k_list = DE_num_calc(args, adata)\n",
    "    print(\"optimized de list = \", de_top_k_list)\n",
    "    with open('./cache/' + args.data_dir.split('/')[-2] + '.txt', 'a+') as fp:\n",
    "        # fp.write('de list: ')\n",
    "        fp.write(','.join([str(i) for i in de_top_k_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial clustering to impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_list_epoch = []\n",
    "if de_top_k_list != []:\n",
    "    print(\"performing DEGs selection\")\n",
    "    adata_list = []\n",
    "    for de_ in de_top_k_list:\n",
    "        for cluster_n in args.impute_cluster_num:\n",
    "            print(\"cluster_n = \", cluster_n)\n",
    "            GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "            ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                        num_cluster=int(cluster_n),\n",
    "                                                                        dif_k=de_, device_id=args.use_gpu_id)\n",
    "            de_list_epoch.append(de_list)\n",
    "            adata_list.append(adata_out)\n",
    "    g_union = set.union(*de_list_epoch)\n",
    "    imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "else:\n",
    "    print(\"skip performing DEGs selection\")\n",
    "    imputed_ad = adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"result of imputed data\"\"\"\n",
    "GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "aris = []\n",
    "print('Dataset:', args.input_data)\n",
    "print('ARI:', ARI)\n",
    "aris.append(ARI)\n",
    "print(aris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
